{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY SOLUTIONS\n",
    "This python notebook evaluates and compares the solutions for the workshop example. There are three solutions in this notebook with direct references to code we've seen in Python notebooks I and II.\n",
    "\n",
    "Each solution shares the same \"Setup\", which will be highlighted below, and even the setup will have its first mentions tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "All solutions will have this setup code required, especially if the solution is being presented in a different cell or notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/macodrum-coding-dh/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install pandas\n",
    "# Installing libraries is not a requirment for the solution to funciton, but it's good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cbrou/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 1-2\n",
    "#The practice of importing libraries and collections are mentioned there. Specific items such as NLTK's collections and pandas are mentioned in Notebook II across multiple cells. \n",
    "\n",
    "nltk.download('stopwords')\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 11-25\n",
    "#Stopwords are further streamlined in Notebook II, code block 12 (NLTK) as a shortcut to that first one.\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "#First Appearance: Notebook II, block 2 (Pandas) lines 3-4.\n",
    "#Completely optional, limits the amount of rows viewed whenever a csv is being printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Creator</th>\n",
       "      <th>Supplement to</th>\n",
       "      <th>Dissertation</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Subject.1</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Description</th>\n",
       "      <th>Contents</th>\n",
       "      <th>Uniform title</th>\n",
       "      <th>...</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Creation Date</th>\n",
       "      <th>Format</th>\n",
       "      <th>Edition</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>General note</th>\n",
       "      <th>Local note</th>\n",
       "      <th>Source</th>\n",
       "      <th>Bound with</th>\n",
       "      <th>Permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Visual Text Analysis in Digital Humanities</td>\n",
       "      <td>J√§nicke, S ;  Franzini, G ;  Cheema, M. F ;  S...</td>\n",
       "      <td>Computer graphics forum, 2017-09, Vol.36 (6), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close reading ;  digital humanities ;  distant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In 2005, Franco Moretti introduced Distant Rea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Wiley Subscription Services, Inc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wiley-Blackwell Full Collection - CRKN; Busine...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASS Digital Humanities Feature: An Introduction</td>\n",
       "      <td>Reischl, Katherine¬†M.H</td>\n",
       "      <td>Canadian-American Slavic studies, 2021-03-25, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alma/SFX Local Collection</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Defining Digital Theology: Digital Humanities,...</td>\n",
       "      <td>Phillips, Peter ;  Schiefelbein-Guerrero, Kyle...</td>\n",
       "      <td>Open theology, 2019-05-22, Vol.5 (1), p.29-43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CODEC ;  Computing for Humanities ;  digital c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This article seeks to define Digital Theology,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>De Gruyter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>De Gruyter Open Access Journals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Distribution features and intellectual structu...</td>\n",
       "      <td>Wang, Qing</td>\n",
       "      <td>Journal of documentation, 2018-01-08, Vol.74 (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Author productivity ;  Bibliometrics ;  Cultur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Purpose The purpose of this paper is to conduc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Bradford: Emerald Group Publishing Limited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computer Science Database</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the Meanings of Self-Regulation: Digital Hu...</td>\n",
       "      <td>Burman, Jeremy T ;  Green, Christopher D ;  Sh...</td>\n",
       "      <td>Child development, 2015-09, Vol.86 (5), p.1507...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Controlled vocabularies ;  EMPIRICAL ARTICLES ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Self-regulation is of interest both to psychol...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>United States: Blackwell Publishing Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wiley-Blackwell Full Collection - CRKN; MEDLIN...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>From provider to partner: how digital humaniti...</td>\n",
       "      <td>Houghton, Chris ;  Ketchley, Sarah</td>\n",
       "      <td>Insights the UKSG journal, 2019-10-16, Vol.32 (1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>academic publishing ;  Alliances and partnersh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The past decade has seen huge growth in the te...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Ubiquity Press Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gale Academic OneFile; Advanced Technologies &amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Digital Humanities and Natural Language Proces...</td>\n",
       "      <td>McGillivray, Barbara ;  Poibeau, Thierry ;  Ru...</td>\n",
       "      <td>Digital humanities quarterly, 2020-07, Vol.14 (2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Computation and Language ;  Computer Science ;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In spite of the increasingly large textual dat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Alliance of Digital Humanities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Freely Accessible Arts &amp; Humanities Journals; ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Keynote 3: Digital Libraries and Digital Human...</td>\n",
       "      <td>Hadi, Widad El</td>\n",
       "      <td>Pakistan journal of information management &amp; l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Analysis ;  Communities of practice ;  digital...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper addresses the relationship between ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Asianet-Pakistan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gale Academic OneFile; Computer Science Databa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Computer Vision and the Digital Humanities: Ad...</td>\n",
       "      <td>Musik, Christoph ;  Zeppelzauer, Matthias</td>\n",
       "      <td>View (Utrecht), 2018-12-31, Vol.7 (14), p.59-72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Active Learning ;  Computer Vision ;  Digital ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Automated computer vision methods and tools of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Netherlands Institute for Sound and Vision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alma/SFX Local Collection; Film &amp; Television L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Digital Journeys @ UOW Australia: From Digital...</td>\n",
       "      <td>Grant, Ren√©e C ;  Organ, Michael</td>\n",
       "      <td>The International information &amp; library review...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Community of practice ;  digital dexterity ;  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In 2018, the University of Wollongong (UOW) la...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Taylor &amp; Francis Library SSH - CRKN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://ocul-crl.primo.exlibrisgroup.com/perma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0          Visual Text Analysis in Digital Humanities   \n",
       "1    CASS Digital Humanities Feature: An Introduction   \n",
       "2   Defining Digital Theology: Digital Humanities,...   \n",
       "3   Distribution features and intellectual structu...   \n",
       "4   On the Meanings of Self-Regulation: Digital Hu...   \n",
       "..                                                ...   \n",
       "45  From provider to partner: how digital humaniti...   \n",
       "46  Digital Humanities and Natural Language Proces...   \n",
       "47  Keynote 3: Digital Libraries and Digital Human...   \n",
       "48  Computer Vision and the Digital Humanities: Ad...   \n",
       "49  Digital Journeys @ UOW Australia: From Digital...   \n",
       "\n",
       "                                              Creator  \\\n",
       "0   J√§nicke, S ;  Franzini, G ;  Cheema, M. F ;  S...   \n",
       "1                              Reischl, Katherine¬†M.H   \n",
       "2   Phillips, Peter ;  Schiefelbein-Guerrero, Kyle...   \n",
       "3                                          Wang, Qing   \n",
       "4   Burman, Jeremy T ;  Green, Christopher D ;  Sh...   \n",
       "..                                                ...   \n",
       "45                 Houghton, Chris ;  Ketchley, Sarah   \n",
       "46  McGillivray, Barbara ;  Poibeau, Thierry ;  Ru...   \n",
       "47                                     Hadi, Widad El   \n",
       "48          Musik, Christoph ;  Zeppelzauer, Matthias   \n",
       "49                   Grant, Ren√©e C ;  Organ, Michael   \n",
       "\n",
       "                                        Supplement to  Dissertation  \\\n",
       "0   Computer graphics forum, 2017-09, Vol.36 (6), ...           NaN   \n",
       "1   Canadian-American Slavic studies, 2021-03-25, ...           NaN   \n",
       "2       Open theology, 2019-05-22, Vol.5 (1), p.29-43           NaN   \n",
       "3   Journal of documentation, 2018-01-08, Vol.74 (...           NaN   \n",
       "4   Child development, 2015-09, Vol.86 (5), p.1507...           NaN   \n",
       "..                                                ...           ...   \n",
       "45  Insights the UKSG journal, 2019-10-16, Vol.32 (1)           NaN   \n",
       "46  Digital humanities quarterly, 2020-07, Vol.14 (2)           NaN   \n",
       "47  Pakistan journal of information management & l...           NaN   \n",
       "48    View (Utrecht), 2018-12-31, Vol.7 (14), p.59-72           NaN   \n",
       "49  The International information & library review...           NaN   \n",
       "\n",
       "                                              Subject  Subject.1  Genre  \\\n",
       "0   close reading ;  digital humanities ;  distant...        NaN    NaN   \n",
       "1                                                 NaN        NaN    NaN   \n",
       "2   CODEC ;  Computing for Humanities ;  digital c...        NaN    NaN   \n",
       "3   Author productivity ;  Bibliometrics ;  Cultur...        NaN    NaN   \n",
       "4   Controlled vocabularies ;  EMPIRICAL ARTICLES ...        NaN    NaN   \n",
       "..                                                ...        ...    ...   \n",
       "45  academic publishing ;  Alliances and partnersh...        NaN    NaN   \n",
       "46  Computation and Language ;  Computer Science ;...        NaN    NaN   \n",
       "47  Analysis ;  Communities of practice ;  digital...        NaN    NaN   \n",
       "48  Active Learning ;  Computer Vision ;  Digital ...        NaN    NaN   \n",
       "49  Community of practice ;  digital dexterity ;  ...        NaN    NaN   \n",
       "\n",
       "                                          Description  Contents  \\\n",
       "0   In 2005, Franco Moretti introduced Distant Rea...       NaN   \n",
       "1                                                 NaN       NaN   \n",
       "2   This article seeks to define Digital Theology,...       NaN   \n",
       "3   Purpose The purpose of this paper is to conduc...       NaN   \n",
       "4   Self-regulation is of interest both to psychol...       NaN   \n",
       "..                                                ...       ...   \n",
       "45  The past decade has seen huge growth in the te...       NaN   \n",
       "46  In spite of the increasingly large textual dat...       NaN   \n",
       "47  This paper addresses the relationship between ...       NaN   \n",
       "48  Automated computer vision methods and tools of...       NaN   \n",
       "49  In 2018, the University of Wollongong (UOW) la...       NaN   \n",
       "\n",
       "    Uniform title  ...                                   Publisher  \\\n",
       "0             NaN  ...            Wiley Subscription Services, Inc   \n",
       "1             NaN  ...                                         NaN   \n",
       "2             NaN  ...                                  De Gruyter   \n",
       "3             NaN  ...  Bradford: Emerald Group Publishing Limited   \n",
       "4             NaN  ...     United States: Blackwell Publishing Ltd   \n",
       "..            ...  ...                                         ...   \n",
       "45            NaN  ...                          Ubiquity Press Ltd   \n",
       "46            NaN  ...              Alliance of Digital Humanities   \n",
       "47            NaN  ...                            Asianet-Pakistan   \n",
       "48            NaN  ...  Netherlands Institute for Sound and Vision   \n",
       "49            NaN  ...                                   Routledge   \n",
       "\n",
       "   Creation Date  Format  Edition Frequency  General note  Local note  \\\n",
       "0            NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "1            NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "2            NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "3            NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "4            NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "..           ...     ...      ...       ...           ...         ...   \n",
       "45           NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "46           NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "47           NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "48           NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "49           NaN     NaN      NaN       NaN           NaN         NaN   \n",
       "\n",
       "                                               Source  Bound with  \\\n",
       "0   Wiley-Blackwell Full Collection - CRKN; Busine...         NaN   \n",
       "1                           Alma/SFX Local Collection         NaN   \n",
       "2                     De Gruyter Open Access Journals         NaN   \n",
       "3                           Computer Science Database         NaN   \n",
       "4   Wiley-Blackwell Full Collection - CRKN; MEDLIN...         NaN   \n",
       "..                                                ...         ...   \n",
       "45  Gale Academic OneFile; Advanced Technologies &...         NaN   \n",
       "46  Freely Accessible Arts & Humanities Journals; ...         NaN   \n",
       "47  Gale Academic OneFile; Computer Science Databa...         NaN   \n",
       "48  Alma/SFX Local Collection; Film & Television L...         NaN   \n",
       "49                Taylor & Francis Library SSH - CRKN         NaN   \n",
       "\n",
       "                                            Permalink  \n",
       "0   https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "1   https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "2   https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "3   https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "4   https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "..                                                ...  \n",
       "45  https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "46  https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "47  https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "48  https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "49  https://ocul-crl.primo.exlibrisgroup.com/perma...  \n",
       "\n",
       "[50 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that variables are declared throughout all code in both notebooks, they can be ...variable to user preference.\n",
    "\n",
    "litrev_df = pd.read_csv('digihum-lit-rev.csv', delimiter=\",\")\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28\n",
    "#This is actually the pandas application of the same fucntion, first seen in Notebook II, code block 2 (Pandas) line 7\n",
    "\n",
    "litrev_df\n",
    "#First Appearance: Notebook II, block 2 (Pandas) line 8\n",
    "#Declaring a (done in the line above) is mentioned on multiple occasions, it is basically a pratice or indicator that a variable or bucket has the file attached and readable for further analysis. \n",
    "#Though optional, it's perfect for learning what the column names are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the minimum requirements for all solutions, or the \"Setup\" for them. \n",
    "\n",
    "# SOLUTION A\n",
    "\n",
    "This solution is a \"Brute force\" method, where the .csv file is forced out as a .txt file and there isn't much preprocessing involved. You will get from Point A to B without inlcusion of stopwords or punctuation checking. This is a very NLTK oriented approach. Though this isn't a clean \"solution\", it can help you figure out your next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/cbrou/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "/var/folders/dy/ld27trl10w9d_gn8dgm6_57m0000gn/T/ipykernel_16038/2978045706.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  litrev_df[\"Description\"] = litrev_df[\"Description\"].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 1435),\n",
       " ('the', 473),\n",
       " (';', 442),\n",
       " ('and', 364),\n",
       " ('of', 325),\n",
       " ('.', 245),\n",
       " (\"''\", 241),\n",
       " ('digital', 232),\n",
       " (':', 231),\n",
       " ('humanities', 194)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "#First appearance: Notebook II, code block 3 (NLTK), required for tokenization. \n",
    "#PUNKT is a resouce part of the nltk library that contains many punctuation tools. It's very useful to not include punctuation in various analyses or text frequencies.\n",
    "\n",
    "from nltk import FreqDist\n",
    "#First appearance: Notebook II, code block 7 (NLTK), required to run a frequency distribution. \n",
    "#FreqDist is a set of functions and tools that streamlines the act of operating on a Frequency Distribution\n",
    "\n",
    "description_list = list(litrev_df[\"Description\"])\n",
    "#First Appearance: Notebook II, block 4 (Pandas), creates a new variable that looks at the dataframe items as a list, easier for export.\n",
    "\n",
    "litrev_df = litrev_df.loc[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "litrev_df[\"Description\"] = litrev_df[\"Description\"].str.lower()\n",
    "#First Appearance: Notebook II, block 13 (Pandas), changes all text to lowercase strings to help NLTK work with it. \n",
    "\n",
    "litrev_df.to_csv(\"test.txt\")\n",
    "#First Appearance: Notebook II, block 15 (Pandas), forces a file to export as a .txt file for easier NLTK reading\n",
    "\n",
    "description = open('test.txt', encoding=\"utf-8\").read().lower()\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28, text exclusive\n",
    "\n",
    "description_words = word_tokenize(description)\n",
    "#First Appearance: Notebook II, block 5 (NLTK), demonstartion of tokenizing words\n",
    "\n",
    "descriptionfrequency = FreqDist(description_words)\n",
    "descriptionfrequency.most_common(10)\n",
    "#First Appearance: Notebook II, block 7 (NLTK), using the frequency distribution functions in NLTK to get a pretty, mediocre result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION B\n",
    "Here are applications of the more primative loops for actual results. Can be streamlined further though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('digital', 240),\n",
       " ('humanities', 195),\n",
       " ('research', 70),\n",
       " ('dh', 51),\n",
       " ('issn', 45),\n",
       " ('permalink', 39),\n",
       " ('https', 38),\n",
       " ('ocul', 38),\n",
       " ('crl', 38),\n",
       " ('primo', 38),\n",
       " ('exlibrisgroup', 38),\n",
       " ('com', 38),\n",
       " ('history', 38),\n",
       " ('vol', 37),\n",
       " ('library', 37),\n",
       " ('analysis', 36),\n",
       " ('p', 36),\n",
       " ('doi', 36),\n",
       " ('eissn', 35),\n",
       " ('text', 33),\n",
       " ('information', 29),\n",
       " ('data', 29),\n",
       " ('collection', 28),\n",
       " ('new', 28),\n",
       " ('journals', 27),\n",
       " ('methods', 26),\n",
       " ('knowledge', 24),\n",
       " ('studies', 24),\n",
       " ('libraries', 23),\n",
       " ('tools', 22),\n",
       " ('computer', 21),\n",
       " ('article', 20),\n",
       " ('design', 20),\n",
       " ('paper', 20),\n",
       " ('scholars', 19),\n",
       " ('media', 19),\n",
       " ('work', 18),\n",
       " ('academic', 18),\n",
       " ('projects', 17),\n",
       " ('science', 17),\n",
       " ('social', 17),\n",
       " ('language', 16),\n",
       " ('two', 16),\n",
       " ('crkn', 15),\n",
       " ('university', 15),\n",
       " ('access', 15),\n",
       " ('field', 15),\n",
       " ('historical', 15),\n",
       " ('education', 15),\n",
       " ('project', 15)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 1-2, the only mention and usage of the regex library and the collections library with the coutner function. This is later replaced by the Frequency Distribution functions in NLTK\n",
    "\n",
    "description_list = list(litrev_df[\"Description\"])\n",
    "#First Appearance: Notebook II, block 4 (Pandas), creates a new variable that looks at the dataframe items as a list, easier for export.\n",
    "\n",
    "litrev_df = litrev_df.loc[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "litrev_df[\"Description\"] = litrev_df[\"Description\"].str.lower()\n",
    "#First Appearance: Notebook II, block 13 (Pandas), changes all text to lowercase strings to help NLTK work with it. \n",
    "\n",
    "litrev_df.to_csv(\"test.txt\")\n",
    "#First Appearance: Notebook II, block 15 (Pandas), forces a file to export as a .txt file for easier NLTK reading\n",
    "\n",
    "description = open('test.txt', encoding=\"utf-8\").read().lower()\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28, text exclusive\n",
    "\n",
    "\n",
    "word_count = 50\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 8. This is a variable cap for the amount of words that can be viewed in the output\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "             'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "             'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', \n",
    "             'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'said', \n",
    "             'say', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    "             'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', \n",
    "             'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "             'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'would', 'could', 'should', \n",
    "             'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', \n",
    "             'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \n",
    "             \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
    "             \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \n",
    "             \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 11-25. This is a brute stopwords list to be used instead of the NLTK dictionary. \n",
    "\n",
    "text_split_into_words = re.split(r\"\\W+\", description)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 31. This uses the regex (Import re) library's split function to put the text file into words that are countable. \n",
    "\n",
    "significant_words = []\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 35. This is the first use of a an empty list, to be used in a loop.\n",
    "\n",
    "for word in text_split_into_words:\n",
    "    if word not in stop_words and word.isalpha():\n",
    "        significant_words.append(word)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 37-39. This is the first loop that processes and updates the list, this also checks if the words are actually words. Thus creating a dictionary or a list of terms.\n",
    "\n",
    "significant_words_tally = Counter(significant_words)\n",
    "order_significant_words = significant_words_tally.most_common(word_count)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 42-45. This tallys and counts up the words, to display them by most common using tools found in the collections library. This is later replaced by NLTK's Frequency Distribution functions. \n",
    "\n",
    "order_significant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION C\n",
    "\n",
    "There is more pre-processing involved which makes this more functional and the ideal middle ground of coding comprehension, problem solving, and creativity üëè!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/cbrou/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cbrou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 1435),\n",
       " ('the', 473),\n",
       " (';', 442),\n",
       " ('and', 364),\n",
       " ('of', 325),\n",
       " ('.', 245),\n",
       " (\"''\", 241),\n",
       " ('digital', 232),\n",
       " (':', 231),\n",
       " ('humanities', 194)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#First appearance: Notebook II, code block 3 (NLTK), required for tokenization and stopwords library\n",
    "\n",
    "from nltk import FreqDist\n",
    "#First appearance: Notebook II, code block 7 (NLTK), required to run a frequency distribution. \n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "#First Appearance: Notebook II, block 8 (NLTK), lines 4-5, adds a list of punctuation to ignore in the frequency distribution\n",
    "\n",
    "description_list = list(litrev_df[\"Description\"])\n",
    "#First Appearance: Notebook II, block 4 (Pandas), creates a new variable that looks at the dataframe items as a list, easier for export.\n",
    "\n",
    "litrev_df = litrev_df.loc[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "litrev_df[\"Description\"] = litrev_df[\"Description\"].str.lower()\n",
    "#First Appearance: Notebook II, block 13 (Pandas), changes all text to lowercase strings to help NLTK work with it. \n",
    "\n",
    "litrev_df.to_csv(\"test.txt\")\n",
    "#First Appearance: Notebook II, block 15 (Pandas), forces a file to export as a .txt file for easier NLTK reading\n",
    "\n",
    "description = open('test.txt', encoding=\"utf-8\").read().lower()\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28, text exclusive\n",
    "\n",
    "description_words = word_tokenize(description)\n",
    "#First Appearance: Notebook II, block 5 (NLTK), demonstartion of tokenizing words\n",
    "\n",
    "## HERE IS WHERE THE MAJOR DIFFERENCES ARE\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#First Appearance: Notebook II, block 6 (NLTK), declaring the stopwords list as a variable for a loop and declaring using the english dictionary\n",
    "\n",
    "stop_words.extend([\"n't\", \"'s\", 'would',])\n",
    "#First Appearance: Notebook II, block 8 (NLTK), line 10, adds more words to the stopword list\n",
    "\n",
    "filtered_transcript_words = []\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 35. this creates and empty list to store all the processed words in\n",
    "\n",
    "for word in description_words:\n",
    "    if word not in stop_words and word not in punctuation:\n",
    "        filtered_transcript_words.append(word)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 37-39, Perfected in Notebook II, block 9 (NLTK), lines 2-5, filters through words removing stopwords and punctuations to create a clean list\n",
    "\n",
    "descriptionfrequency = FreqDist(description_words)\n",
    "descriptionfrequency.most_common(10)\n",
    "#First Appearance: Notebook II, block 7 (NLTK), using the frequency distribution functions in NLTK to get a pretty, mediocre result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION D\n",
    "\n",
    "Chantal's original solution, a full display of using libraries to their maximum potential! This is above and beyond so we don't expect you to get here, but will be very impressed if you do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniqueWords\n",
       "digital          138\n",
       "humanities       101\n",
       "research          54\n",
       "dh                48\n",
       "analysis          27\n",
       "                ... \n",
       "open               1\n",
       "consider           1\n",
       "peers              1\n",
       "assessing          1\n",
       "applicability      1\n",
       "Name: count, Length: 1762, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litrev_df = litrev_df[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "abstracts_as_text = \"\"\n",
    "#First Appearance: Notebook I, block 4 (Naming Variables), examines how to declare an empty string, but there is no further mention of this as an application\n",
    "\n",
    "for i in litrev_df[\"Description\"]:\n",
    "    abstracts_as_text = abstracts_as_text + i + \"\\n\"\n",
    "#First Appearance: Notebook II, block 3 (Loops) line 5. This is the first demonstration of concatenation in a loop. \n",
    "    \n",
    "abstractTokens = word_tokenize(abstracts_as_text.lower())\n",
    "#MULTIPLE APPEARANCES - word_tokenize uses code from Notebook II, block 5 (NLTK), abstracts_as_text is a variable and the .lower() function for the variable is used when reading files and converting cells to strings in  Notebook II, block 13 (Pandas).\n",
    "\n",
    "cleaned_abstractTokens = []\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 35. This is the first use of a an empty list, to be used in a loop.\n",
    "\n",
    "for word in list(abstractTokens):\n",
    "    if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "        cleaned_abstractTokens.append(word)\n",
    "#MULTIPLE APPEARANCES - This is a cleaned up combination of the loops found in Notebook I, block 1 (A simple python script) line 37-39 AND Notebook II, block 9 (NLTK), lines 2-5. The cobination is done using the stopwords.words('english') dictionary, and checking if a word.isalpha() instead of using punctuation.\n",
    "\n",
    "abstracts_df = pd.DataFrame(cleaned_abstractTokens, columns =['uniqueWords'])\n",
    "#NO previous appearance - This is a line of code that creates a new dataframe to house all the cleaned tokens from the previous loop, this places those tokens (words) ina  a column named \"uniqueWords\", this is part of that vital gap connecting using pandas dataframe cells in NLTK.\n",
    "        \n",
    "keywords = abstracts_df[\"uniqueWords\"].value_counts()\n",
    "#First Appearance: Notebook II, block 9 (Pandas), this counts the values using a pandas function, meaning that this version never uses LTK besides the tokenizations and stopwords libraries. \n",
    "\n",
    "keywords\n",
    "#Declaring the variable for printing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macodrum-coding-dh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
